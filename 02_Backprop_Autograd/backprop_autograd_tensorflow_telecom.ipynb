{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Telecom Traffic Forecasting with TensorFlow GradientTape  \n",
        "*A practical demonstration of backpropagation & autograd in a custom training loop.*\n",
        "\n",
        "This notebook builds a small but realistic telecom-style forecasting experiment using TensorFlow 2.x.  \n",
        "The focus is on:\n",
        "\n",
        "- synthetic yet meaningful traffic data  \n",
        "- multi-feature regression (hour, weekday, previous traffic)  \n",
        "- a DNN trained with a **manually written training loop**  \n",
        "- explicit backprop via **`tf.GradientTape()`**  \n",
        "- metrics & validation splits"
      ],
      "metadata": {
        "id": "GsxqQM-rV7Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports & Seeds\n",
        "\n",
        "We keep everything deterministic to make training runs reproducible."
      ],
      "metadata": {
        "id": "QVi5po5LWh8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "AY4K0lrEWBRd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Synthetic Telecom Data  \n",
        "Real-world telecom traffic usually shows:\n",
        "\n",
        "- daily periodicity (rush hours)\n",
        "- weekly periodicity (weekday vs weekend)\n",
        "- lag dependence (previous hour influences current hour)\n",
        "- noise from unpredictable events\n",
        "\n",
        "We simulate **120 days of hourly data** and engineer three input features:\n",
        "\n",
        "**Features**\n",
        "- hour_of_day (0–23)\n",
        "- day_of_week (0–6)\n",
        "- prev_hour_traffic (lag 1)\n",
        "\n",
        "**Target**\n",
        "- current hour traffic"
      ],
      "metadata": {
        "id": "ikHN9kwSWJli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_days = 120\n",
        "hours_per_day = 24\n",
        "n_samples = num_days * hours_per_day\n",
        "\n",
        "hours = np.tile(np.arange(hours_per_day), num_days)\n",
        "days = np.repeat(np.arange(num_days) % 7, hours_per_day)\n",
        "\n",
        "# Seasonality patterns\n",
        "daily_pattern = 10 + 5 * np.sin(2 * np.pi * hours / 24.0)\n",
        "weekly_pattern = 2 * np.sin(2 * np.pi * days / 7.0)\n",
        "\n",
        "noise = np.random.normal(scale=1.0, size=n_samples)\n",
        "\n",
        "traffic = daily_pattern + weekly_pattern + noise\n",
        "\n",
        "prev_traffic = np.concatenate([[traffic[0]], traffic[:-1]])\n",
        "\n",
        "# Stack features\n",
        "X = np.stack(\n",
        "    [\n",
        "        hours / 23.0,\n",
        "        days / 6.0,\n",
        "        prev_traffic / 20.0\n",
        "    ],\n",
        "    axis=1\n",
        ").astype(np.float32)\n",
        "\n",
        "y = traffic.astype(np.float32).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "jnt3SaF7Wkqc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train–Test Split and Data Pipeline  \n",
        "We use an 80/20 split and build a `tf.data.Dataset` pipeline for efficient batching."
      ],
      "metadata": {
        "id": "7KYA35lwWoKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(0.8 * n_samples)\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    .shuffle(1000)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "\n",
        "test_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "    .batch(batch_size)\n",
        ")\n"
      ],
      "metadata": {
        "id": "G2qi1eHUWl-F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Definition  \n",
        "A lightweight DNN:\n",
        "\n",
        "Input(3) → Dense(32, relu) → Dense(16, relu) → Dense(1)\n",
        "\n",
        "\n",
        "This keeps the architecture simple but expressive enough for a seasonal time-series regression task.\n"
      ],
      "metadata": {
        "id": "YAr7J7DzWrvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    return tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Input(shape=(3,)),\n",
        "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "model = build_model()\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "train_mae = tf.keras.metrics.MeanAbsoluteError(name=\"train_mae\")\n",
        "val_mae = tf.keras.metrics.MeanAbsoluteError(name=\"val_mae\")\n"
      ],
      "metadata": {
        "id": "jBm9rbWtWp3_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Custom Training Step (Backprop via GradientTape)  \n",
        "This is the key part of the project.\n",
        "\n",
        "We do **not** use `model.fit()`.\n",
        "\n",
        "Instead, we explicitly:\n",
        "\n",
        "1. open a `tf.GradientTape()`  \n",
        "2. run the forward pass  \n",
        "3. compute the loss  \n",
        "4. backpropagate with `tape.gradient()`  \n",
        "5. update parameters with `optimizer.apply_gradients()`\n",
        "\n",
        "This mirrors exactly what *backpropagation* does inside TensorFlow’s autograd engine.\n"
      ],
      "metadata": {
        "id": "c3M_Log-Wy1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        preds = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, preds)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    train_mae.update_state(y_batch, preds)\n",
        "    return loss\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def val_step(x_batch, y_batch):\n",
        "    preds = model(x_batch, training=False)\n",
        "    loss = loss_fn(y_batch, preds)\n",
        "    val_mae.update_state(y_batch, preds)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "T0pLqv2vWx0t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Training Loop  \n",
        "A production-style loop:\n",
        "\n",
        "- resets metrics per epoch  \n",
        "- accumulates batch losses  \n",
        "- prints validation metrics  \n",
        "- no Keras shortcuts  \n",
        "\n",
        "This shows understanding of how *GradientTape enables backprop* in TensorFlow.\n"
      ],
      "metadata": {
        "id": "D9q8OcKZW2AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_mae.reset_state()\n",
        "    val_mae.reset_state()\n",
        "\n",
        "    train_losses = []\n",
        "    for xb, yb in train_ds:\n",
        "        loss = train_step(xb, yb)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "    val_losses = []\n",
        "    for xb, yb in test_ds:\n",
        "        vloss = val_step(xb, yb)\n",
        "        val_losses.append(vloss)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train_loss={tf.reduce_mean(train_losses):.4f} \"\n",
        "        f\"val_loss={tf.reduce_mean(val_losses):.4f} \"\n",
        "        f\"train_MAE={train_mae.result():.3f} \"\n",
        "        f\"val_MAE={val_mae.result():.3f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzaGagNKW1G1",
        "outputId": "42c6a89a-6d0a-497a-be14-6e34a7c64b0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=106.8639 val_loss=94.3992 train_MAE=9.579 val_MAE=8.880\n",
            "Epoch 02 | train_loss=85.8152 val_loss=69.3079 train_MAE=8.389 val_MAE=7.327\n",
            "Epoch 03 | train_loss=56.7871 val_loss=38.9817 train_MAE=6.475 val_MAE=5.147\n",
            "Epoch 04 | train_loss=28.7186 val_loss=19.7406 train_MAE=4.385 val_MAE=3.717\n",
            "Epoch 05 | train_loss=16.7523 val_loss=15.2202 train_MAE=3.411 val_MAE=3.268\n",
            "Epoch 06 | train_loss=13.2755 val_loss=12.1585 train_MAE=3.042 val_MAE=2.909\n",
            "Epoch 07 | train_loss=10.4924 val_loss=9.4509 train_MAE=2.691 val_MAE=2.556\n",
            "Epoch 08 | train_loss=8.1055 val_loss=7.2502 train_MAE=2.355 val_MAE=2.224\n",
            "Epoch 09 | train_loss=6.1891 val_loss=5.5290 train_MAE=2.045 val_MAE=1.924\n",
            "Epoch 10 | train_loss=4.7496 val_loss=4.2743 train_MAE=1.779 val_MAE=1.680\n",
            "Epoch 11 | train_loss=3.7678 val_loss=3.4512 train_MAE=1.579 val_MAE=1.507\n",
            "Epoch 12 | train_loss=3.1405 val_loss=2.9239 train_MAE=1.440 val_MAE=1.389\n",
            "Epoch 13 | train_loss=2.7613 val_loss=2.6156 train_MAE=1.351 val_MAE=1.314\n",
            "Epoch 14 | train_loss=2.5459 val_loss=2.4449 train_MAE=1.296 val_MAE=1.270\n",
            "Epoch 15 | train_loss=2.4283 val_loss=2.3331 train_MAE=1.264 val_MAE=1.234\n",
            "Epoch 16 | train_loss=2.3525 val_loss=2.2676 train_MAE=1.242 val_MAE=1.211\n",
            "Epoch 17 | train_loss=2.3034 val_loss=2.2250 train_MAE=1.226 val_MAE=1.200\n",
            "Epoch 18 | train_loss=2.2671 val_loss=2.1788 train_MAE=1.216 val_MAE=1.184\n",
            "Epoch 19 | train_loss=2.2363 val_loss=2.1459 train_MAE=1.206 val_MAE=1.174\n",
            "Epoch 20 | train_loss=2.2025 val_loss=2.1141 train_MAE=1.197 val_MAE=1.165\n",
            "Epoch 21 | train_loss=2.1725 val_loss=2.0910 train_MAE=1.187 val_MAE=1.160\n",
            "Epoch 22 | train_loss=2.1557 val_loss=2.0629 train_MAE=1.182 val_MAE=1.152\n",
            "Epoch 23 | train_loss=2.1236 val_loss=2.0372 train_MAE=1.172 val_MAE=1.145\n",
            "Epoch 24 | train_loss=2.1028 val_loss=2.0123 train_MAE=1.166 val_MAE=1.138\n",
            "Epoch 25 | train_loss=2.0844 val_loss=1.9934 train_MAE=1.160 val_MAE=1.132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Sample Predictions  \n",
        "A quick qualitative check to see if the model learned the shape of the traffic pattern.\n"
      ],
      "metadata": {
        "id": "uOJoykAQW51g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = X_test[:5]\n",
        "pred = model(sample, training=False)\n",
        "\n",
        "print(\"\\nSample predictions (true vs pred):\")\n",
        "for true_y, pred_y in zip(y_test[:5], pred.numpy()):\n",
        "    print(f\"true={true_y[0]:6.2f}  pred={pred_y[0]:6.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDxrHGZHW340",
        "outputId": "87730a95-7fac-418b-e769-3a5cc04266c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions (true vs pred):\n",
            "true= 10.05  pred= 10.26\n",
            "true= 12.45  pred= 11.44\n",
            "true= 11.16  pred= 12.95\n",
            "true= 11.40  pred= 11.81\n",
            "true= 12.91  pred= 11.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Summary\n",
        "\n",
        "The key objective was to demonstrate **backpropagation and gradient flow** through a realistic regression model using:\n",
        "\n",
        "- TensorFlow's autograd system (`GradientTape`)\n",
        "- custom training & validation loops\n",
        "- synthetic telecom-style traffic data\n",
        "- meaningful feature engineering\n",
        "- proper metrics and validation handling\n"
      ],
      "metadata": {
        "id": "hEioboFlW9SF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SxA0y27dW7L8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}