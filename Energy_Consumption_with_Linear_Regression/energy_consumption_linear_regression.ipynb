import nbformat as nbf

# Create a new notebook
nb = nbf.v4.new_notebook()

cells = []

# 1. Title + intro
cells.append(nbf.v4.new_markdown_cell("""# ðŸ”‹ Predicting Home Appliances Energy Consumption with Linear Regression

This beginner-friendly notebook shows how to use **Ordinary Least Squares (OLS) Linear Regression** to predict  
**home appliances energy consumption** from a real Kaggle dataset:

> `KAG_energydata_complete.csv`

We will go step-by-step:

1. Understand the problem in simple words  
2. Load and explore the dataset  
3. Choose features (inputs) and a target (output)  
4. Split the data into train and test sets  
5. Train a Linear Regression model  
6. Evaluate the model with simple metrics  
7. Visualise predictions and errors  
8. Summarise what we learned
"""))

# 2. Problem explanation
cells.append(nbf.v4.new_markdown_cell("""## 1. Problem in plain English

Imagine you work for a smart home or energy company.

You want to estimate **how much energy the home appliances will consume** at a given time, based on things like:

- indoor temperature and humidity  
- outside temperature and weather  
- time of day  
- whether lights are on  

You don't want a complicated black-box model yet.  
You want a **simple, interpretable baseline** that gives a reasonable estimate.

That's exactly what **Linear Regression (OLS)** is good at:

> It tries to fit the **best straight-line relationship** between inputs (features) and the output (target).
"""))

# 3. Imports
cells.append(nbf.v4.new_markdown_cell("""## 2. Setup and imports

First, we import the Python libraries we'll use in this notebook."""))

cells.append(nbf.v4.new_code_cell("""# Basic data libraries
import numpy as np
import pandas as pd

# Plotting
import matplotlib.pyplot as plt

# Machine learning tools from scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Make plots a bit nicer
plt.style.use("seaborn-v0_8")
"""))

# 4. Load dataset
cells.append(nbf.v4.new_markdown_cell("""## 3. Load the Kaggle dataset

We assume the file **`KAG_energydata_complete.csv`** is in the **same folder** as this notebook.

If you are running this in Google Colab, you will need to upload the CSV or mount your Google Drive."""))

cells.append(nbf.v4.new_code_cell("""# Load the dataset
file_path = "KAG_energydata_complete.csv"  # make sure this file is in the same folder
df = pd.read_csv(file_path)

# Quick look at the data
print("Shape of data (rows, columns):", df.shape)
df.head()"""))

# 5. Basic exploration
cells.append(nbf.v4.new_markdown_cell("""## 4. Quick data exploration

Let's understand what columns we have and what they look like.

We will:

- look at the first few rows  
- check basic info (data types, missing values)  
- see basic statistics for numeric columns
"""))

cells.append(nbf.v4.new_code_cell("""# Basic info about columns and data types
df.info()"""))

cells.append(nbf.v4.new_code_cell("""# Basic statistics for numeric columns
df.describe().T"""))

# 6. Explain target and features
cells.append(nbf.v4.new_markdown_cell("""## 5. Choosing target and features

In any supervised ML problem we need:

- **Target (y)** â†’ what we want to predict  
- **Features (X)** â†’ what we use to make the prediction  

For this dataset:

- We will predict **`Appliances`**, which is the energy consumption of appliances (in Wh).  
- We will use a small, understandable set of features:

  - `T1` â†’ kitchen temperature  
  - `RH_1` â†’ kitchen humidity  
  - `T_out` â†’ outside temperature  
  - `Windspeed`  
  - `Visibility`  
  - `lights` â†’ lights energy usage  
  - plus simple time-based features extracted from the `date` column:
    - `hour` â†’ hour of day (0â€“23)  
    - `day_of_week` â†’ day of week (0=Monday, 6=Sunday)
"""))

# 7. Feature engineering
cells.append(nbf.v4.new_markdown_cell("""### 5.1 Create time-based features from the date column"""))

cells.append(nbf.v4.new_code_cell("""# Convert date column to datetime
df['date'] = pd.to_datetime(df['date'])

# Create simple time features
df['hour'] = df['date'].dt.hour
df['day_of_week'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6

# Show the new columns
df[['date', 'hour', 'day_of_week']].head()"""))

cells.append(nbf.v4.new_markdown_cell("""### 5.2 Define features (X) and target (y)"""))

cells.append(nbf.v4.new_code_cell("""# Target column
target_col = 'Appliances'

# Selected feature columns
feature_cols = [
    'T1',          # kitchen temperature
    'RH_1',        # kitchen humidity
    'T_out',       # outside temperature
    'Windspeed',
    'Visibility',
    'lights',
    'hour',
    'day_of_week'
]

# Keep only rows where these columns are not missing
data = df[feature_cols + [target_col]].dropna()

X = data[feature_cols]
y = data[target_col]

print("Number of samples after dropping missing values:", len(data))
print("Features used:", feature_cols)"""))

# 8. Train-test split
cells.append(nbf.v4.new_markdown_cell("""## 6. Train-test split

We split the data into two parts:

- **Training set (80%)** â†’ used to fit (train) the model  
- **Test set (20%)** â†’ kept aside to evaluate how well the model generalises  

We use `train_test_split` from scikit-learn."""))

cells.append(nbf.v4.new_code_cell("""X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Training samples:", X_train.shape[0])
print("Test samples:", X_test.shape[0])"""))

# 9. Train model
cells.append(nbf.v4.new_markdown_cell("""## 7. Training the Linear Regression model

Now we create a **LinearRegression** model and fit it on the training data.

This will learn the best straight-line relationship between our chosen features and the target `Appliances`."""))

cells.append(nbf.v4.new_code_cell("""# Create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

print("Model trained!")"""))

# 10. Evaluation
cells.append(nbf.v4.new_markdown_cell("""## 8. Evaluating the model

To see how good the model is, we:

1. Use it to predict on the **test set** (data it has not seen before).  
2. Compare predictions with the true values.  
3. Compute simple error metrics:

- **MAE (Mean Absolute Error)** â†’ average size of errors  
- **RMSE (Root Mean Squared Error)** â†’ like MAE but penalises big errors more

Smaller values are better.
"""))

cells.append(nbf.v4.new_code_cell("""# Predict on the test set
y_pred = model.predict(X_test)

# Compute metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"MAE:  {mae:.2f} Wh")
print(f"RMSE: {rmse:.2f} Wh")"""))

# 11. Coefficients
cells.append(nbf.v4.new_markdown_cell("""### 8.1 Understanding the model coefficients

Linear Regression learns an equation of the form:

\\begin{align}
\\text{Appliances} &= a_0 + a_1 \\cdot T1 + a_2 \\cdot RH\\_1 + \\dots
\\end{align}

The **coefficients** (`a_1`, `a_2`, â€¦) tell us how the prediction changes when each feature changes.

- Positive coefficient â†’ as the feature increases, predicted energy **increases**  
- Negative coefficient â†’ as the feature increases, predicted energy **decreases**
"""))

cells.append(nbf.v4.new_code_cell("""# Show coefficients in a tidy table
coef_df = pd.DataFrame({
    'feature': feature_cols,
    'coefficient': model.coef_
}).sort_values(by='coefficient', key=np.abs, ascending=False)

coef_df"""))

# 12. Visualisations
cells.append(nbf.v4.new_markdown_cell("""## 9. Visualisations

Plots help us **see** how the model is doing.

We will create:

1. **Feature vs target scatter plots** for a few features  
2. **Actual vs Predicted** plot on the test set  
3. **Residuals histogram** (errors distribution)
"""))

# 12.1 Feature vs target plots
cells.append(nbf.v4.new_markdown_cell("""### 9.1 Feature vs target scatter plots

These plots show how the target (`Appliances`) relates to each feature individually.

- If the points look like a slanted cloud â†’ a line might work well  
- If they look very curved or chaotic â†’ linear regression may struggle
"""))

cells.append(nbf.v4.new_code_cell("""# Pick a few features to visualise
features_to_plot = ['T1', 'RH_1', 'T_out', 'lights']

fig, axes = plt.subplots(1, len(features_to_plot), figsize=(16, 4))

for ax, col in zip(axes, features_to_plot):
    ax.scatter(data[col], data[target_col], alpha=0.3)
    ax.set_xlabel(col)
    ax.set_ylabel(target_col)
    ax.set_title(f"{col} vs {target_col}")

plt.tight_layout()
plt.show()"""))

# 12.2 Actual vs Predicted
cells.append(nbf.v4.new_markdown_cell("""### 9.2 Actual vs Predicted (test set)

Each point is one example from the **test set**:

- x-axis: actual `Appliances` value  
- y-axis: predicted `Appliances` value  
- The dashed diagonal line represents **perfect predictions**

If points cluster near the line, the model is doing reasonably well.
"""))

cells.append(nbf.v4.new_code_cell("""plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, alpha=0.4)
min_val = min(y_test.min(), y_pred.min())
max_val = max(y_test.max(), y_pred.max())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')
plt.xlabel("Actual Appliances energy (Wh)")
plt.ylabel("Predicted Appliances energy (Wh)")
plt.title("Actual vs Predicted (Test Set)")
plt.legend()
plt.tight_layout()
plt.show()"""))

# 12.3 Residuals
cells.append(nbf.v4.new_markdown_cell("""### 9.3 Residuals distribution

**Residual = Actual âˆ’ Predicted**

We want to see:

- Are most residuals close to 0?  
- Are errors spread somewhat symmetrically around 0?  

If there are strong patterns (e.g. two peaks, or very skewed),  
that might mean a simple linear model is missing something important.
"""))

cells.append(nbf.v4.new_code_cell("""residuals = y_test - y_pred

plt.figure(figsize=(6, 4))
plt.hist(residuals, bins=30, edgecolor='black')
plt.xlabel("Residual (Actual - Predicted)")
plt.ylabel("Count")
plt.title("Residuals Distribution")
plt.tight_layout()
plt.show()"""))

# 13. Summary
cells.append(nbf.v4.new_markdown_cell("""## 10. Summary and next steps

In this notebook, you:

1. Loaded a **real Kaggle dataset** (`KAG_energydata_complete.csv`)  
2. Defined a **target** (`Appliances`) and a set of intuitive **features**  
3. Created simple time-based features (`hour`, `day_of_week`)  
4. Split the data into **training** and **test** sets  
5. Trained an **Ordinary Least Squares Linear Regression** model  
6. Evaluated it with **MAE** and **RMSE**  
7. Looked at model **coefficients** to get some interpretability  
8. Visualised predictions and residuals to build intuition

This is a solid **first baseline model** for this kind of problem.

### Possible next steps

If you want to go further, you could:

- Try **Ridge** or **Lasso** regression (adds regularisation)  
- Try **tree-based models** like Random Forest or Gradient Boosting  
- Do more detailed **feature engineering** (e.g. interaction terms, lag features)  
- Treat this as a **time-series** problem and use models like ARIMA or Prophet

But even this simple notebook already shows a full ML workflow:
**from raw CSV â†’ to model â†’ to evaluation â†’ to interpretation â†’ to visualisation.**
"""))

# Attach cells to notebook
nb['cells'] = cells

# Save notebook
output_path = "/mnt/data/energy_regression_kaggle_beginner.ipynb"
with open(output_path, "w", encoding="utf-8") as f:
    nbf.write(nb, f)

output_path
