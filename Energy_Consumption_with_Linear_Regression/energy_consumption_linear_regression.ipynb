"""
Predicting Home Appliances Energy Consumption using Linear Regression (OLS)
Dataset: KAG_energydata_complete.csv (Kaggle)

This script:
- Loads the dataset
- Does a small amount of feature engineering
- Trains a Linear Regression model
- Evaluates with MAE and RMSE
- Plots: Actual vs Predicted and Residuals
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

# --------------------------------------
# 1. Load the dataset
# --------------------------------------
# Make sure the CSV is in the same folder as this script
df = pd.read_csv("KAG_energydata_complete.csv")

# Quick sanity check
print("Shape of data:", df.shape)
print("Columns:", df.columns.tolist()[:10], "...")

# --------------------------------------
# 2. Basic preprocessing & feature engineering
# --------------------------------------

# Convert date column to datetime
df["date"] = pd.to_datetime(df["date"])

# Create simple time-based features (very beginner-friendly)
df["hour"] = df["date"].dt.hour
df["day_of_week"] = df["date"].dt.dayofweek  # Monday=0, Sunday=6

# Target: Appliances energy consumption (Wh)
target_col = "Appliances"

# Select a small, understandable subset of features
feature_cols = [
    "T1",          # Kitchen temp
    "RH_1",       # Kitchen humidity
    "T_out",      # Outside temp
    "Windspeed",
    "Visibility",
    "hour",
    "day_of_week",
    "lights"      # Internal lighting usage
]

# Drop any rows with missing values in selected columns (safe for this dataset)
data = df[feature_cols + [target_col]].dropna()

X = data[feature_cols]
y = data[target_col]

print("\nUsing features:", feature_cols)
print("Number of samples after dropping NA:", len(data))

# --------------------------------------
# 3. Train-test split
# --------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --------------------------------------
# 4. Train Linear Regression (OLS)
# --------------------------------------
model = LinearRegression()
model.fit(X_train, y_train)

# --------------------------------------
# 5. Evaluate model
# --------------------------------------
y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("\n--- Model Evaluation ---")
print(f"MAE:  {mae:.2f} Wh")
print(f"RMSE: {rmse:.2f} Wh")

# Show coefficients with feature names
coef_df = pd.DataFrame({
    "feature": feature_cols,
    "coefficient": model.coef_
}).sort_values(by="coefficient", key=np.abs, ascending=False)

print("\n--- Coefficients (sorted by |value|) ---")
print(coef_df)

# --------------------------------------
# 6. Visualisations
# --------------------------------------

# 6.1 Actual vs Predicted
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
min_val = min(y_test.min(), y_pred.min())
max_val = max(y_test.max(), y_pred.max())
plt.plot([min_val, max_val], [min_val, max_val], "r--", label="Perfect prediction")
plt.xlabel("Actual Appliances Energy (Wh)")
plt.ylabel("Predicted Appliances Energy (Wh)")
plt.title("Actual vs Predicted (Test Set)")
plt.legend()
plt.tight_layout()
plt.show()

# 6.2 Residuals histogram
residuals = y_test - y_pred
plt.figure(figsize=(6, 4))
plt.hist(residuals, bins=30, edgecolor="black")
plt.xlabel("Residual (Actual - Predicted)")
plt.ylabel("Count")
plt.title("Residuals Distribution")
plt.tight_layout()
plt.show()
